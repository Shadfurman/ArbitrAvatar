{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shadfurman/ArbitrAvatar/blob/master/trading_bot_scaffold.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wILlIqQMcY2"
      },
      "source": [
        "# Trading Bot: Conv-Transformer → Backtest (Scaffold)\n",
        "\n",
        "End-to-end notebook scaffold. Fill in the TODOs only—no scope creep.\n",
        "\n",
        "**Sections:**\n",
        "- Setup & seed\n",
        "- Data & features\n",
        "- Split\n",
        "- Model\n",
        "- Train\n",
        "- Inference & backtest\n",
        "- Metrics & plots\n",
        "- Params dump & outputs\n"
      ],
      "id": "5wILlIqQMcY2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsV6n4toMcY8"
      },
      "source": [
        "## Setup & seed"
      ],
      "id": "gsV6n4toMcY8"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "H9Uihs1lMcY-"
      },
      "outputs": [],
      "source": [
        "# --- Setup & seed ---\n",
        "\n",
        "import os, random, json, glob, math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import IterableDataset, DataLoader\n",
        "\n",
        "# Paths (adjust as needed; do not restructure unless necessary)\n",
        "PARQUET_DIR = '/content/drive/MyDrive/stock_market_data/data_parquet'\n",
        "OUTPUT_DIR  = '/content/drive/MyDrive/Colab Notebooks/outputs'\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Hyperparameters and configuration\n",
        "# Define default parameters. You should update values (especially 'features' and date ranges) for your dataset.\n",
        "PARAMS = {\n",
        "    'seed': 42,\n",
        "    'features': ['open', 'high', 'low', 'close', 'volume'],\n",
        "    'lookback': 64,\n",
        "    'batch_size': 512,\n",
        "    'model_option': 'convtransformer',\n",
        "    'split': {\n",
        "        'train_start': '2005-01-01',\n",
        "        'train_end':   '2015-12-31',\n",
        "        'val_start':   '2016-01-01',\n",
        "        'val_end':     '2017-12-31',\n",
        "        'test_start':  '2018-01-01',\n",
        "        'test_end':    '2018-12-31',\n",
        "    },\n",
        "    'calibration_T': 1.0,\n",
        "    'margin': 0.15,\n",
        "    'portfolio_mode': 'long_short',\n",
        "}\n",
        "\n",
        "# Set random seeds\n",
        "SEED = PARAMS.get('seed', 42)\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n"
      ],
      "id": "H9Uihs1lMcY-"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5TEBoq9gxnmk",
        "outputId": "f3223170-f387-4cef-9434-9bf249fcec2a"
      },
      "id": "5TEBoq9gxnmk",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- BOOTSTRAP A: mount + load params/norms + rebuild path lists (no model deps) ---\n",
        "\n",
        "# Artifact paths\n",
        "ART = {\n",
        "    'lock': os.path.join(OUTPUT_DIR, 'params_locked.json'),\n",
        "    'norms': os.path.join(OUTPUT_DIR, 'feature_norm.csv'),\n",
        "    'ckpt': os.path.join(OUTPUT_DIR, 'best_model.pt'),\n",
        "}\n",
        "PRELOAD = {k: os.path.exists(v) for k, v in ART.items()}\n",
        "\n",
        "# Merge locked params if present\n",
        "if PRELOAD['lock']:\n",
        "    with open(ART['lock'], 'r') as f:\n",
        "        locked_params = json.load(f)\n",
        "    PARAMS.update(locked_params)\n",
        "\n",
        "# Load normalization stats if present and define normalize_inplace\n",
        "FEAT_MEAN, FEAT_STD = {}, {}\n",
        "if PRELOAD['norms']:\n",
        "    norm_df = pd.read_csv(ART['norms'], index_col=0)\n",
        "    FEAT_MEAN = norm_df['mean'].astype(float).to_dict()\n",
        "    FEAT_STD  = norm_df['std'].replace(0, 1.0).astype(float).to_dict()\n",
        "    def normalize_inplace(df: pd.DataFrame) -> pd.DataFrame:\n",
        "        for col in PARAMS['features']:\n",
        "            if col in df.columns:\n",
        "                df[col] = (df[col] - FEAT_MEAN.get(col, 0.0)) / FEAT_STD.get(col, 1.0)\n",
        "        return df\n",
        "else:\n",
        "    def normalize_inplace(df: pd.DataFrame) -> pd.DataFrame:\n",
        "        return df\n",
        "\n",
        "# Helper to count rows in a date range for a parquet file (lightweight)\n",
        "def count_rows_in_range(p: str, start: str, end: str, any_col: str | None = None) -> int:\n",
        "    any_col = any_col or (PARAMS['features'] + ['y'])[0]\n",
        "    df = pd.read_parquet(p, columns=[any_col]).sort_index()\n",
        "    df = df.loc[(df.index >= pd.to_datetime(start)) & (df.index <= pd.to_datetime(end))]\n",
        "    return len(df)\n",
        "\n",
        "# Discover parquet files and apply lookback+2 filter on train window\n",
        "all_paths = sorted(glob.glob(os.path.join(PARQUET_DIR, '*.parquet')))\n",
        "S = PARAMS['split']\n",
        "lookback = int(PARAMS['lookback'])\n",
        "\n",
        "# Keep tickers with enough rows in the train window\n",
        "keep_paths = [p for p in all_paths if count_rows_in_range(p, S['train_start'], S['train_end']) >= lookback + 2]\n",
        "\n",
        "def filt(ps, a, b):\n",
        "    return [p for p in ps if count_rows_in_range(p, a, b) >= lookback + 2]\n",
        "\n",
        "train_paths = filt(keep_paths, S['train_start'], S['train_end'])\n",
        "val_paths   = filt(keep_paths, S['val_start'],   S['val_end'])\n",
        "test_paths  = filt(keep_paths, S['test_start'],  S['test_end'])\n",
        "\n",
        "print(\"Bootstrap A →\", f\"lock={PRELOAD['lock']} norms={PRELOAD['norms']} ckpt={PRELOAD['ckpt']} |\",      f\"tickers train/val/test = {len(train_paths)}/{len(val_paths)}/{len(test_paths)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mgnpaJU3Q7uE",
        "outputId": "4202922c-7e87-46f2-d039-695ce91f1681"
      },
      "id": "mgnpaJU3Q7uE",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bootstrap A → lock=True norms=True ckpt=True | tickers train/val/test = 2558/2415/2406\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- LOCAL OVERRIDES (optional, session-only; not persisted) ---\n",
        "# These only affect this runtime unless you later write/lock params.\n",
        "LOCAL_OVERRIDES = {\n",
        "    # portfolio behavior\n",
        "    \"portfolio_mode\": \"long_only\",   # or \"long_short\"\n",
        "    \"margin\": 0.02,\n",
        "\n",
        "    # calibration\n",
        "    \"calibration_T\": PARAMS.get(\"calibration_T\", 1.0),\n",
        "}\n",
        "\n",
        "print(\"LOCAL_OVERRIDES set:\", LOCAL_OVERRIDES)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_JfKrjAJCQO1",
        "outputId": "12d62c3f-76db-4729-dc6b-17de86b46608"
      },
      "id": "_JfKrjAJCQO1",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOCAL_OVERRIDES set: {'portfolio_mode': 'long_only', 'margin': 0.02, 'calibration_T': 0.899}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPueuQsHMcZA"
      },
      "source": [
        "## Data & features"
      ],
      "id": "uPueuQsHMcZA"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "N8hKKGhFMcZB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93a3c2f1-9645-4414-8b0e-d81fa4639aa7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data/feature functions defined. Fill TODOs as needed.\n"
          ]
        }
      ],
      "source": [
        "import yfinance as yf\n",
        "\n",
        "def load_ohlcv_yf(ticker: str, start: str, end: str) -> pd.DataFrame:\n",
        "    \"\"\"Load daily OHLCV from yfinance. TODO: cache to CSV in ./data.\"\"\"\n",
        "    df = yf.download(ticker, start=start, end=end, progress=False)\n",
        "    if df.empty:\n",
        "        raise ValueError(f'No data for {ticker}')\n",
        "    df = df[['Open','High','Low','Close','Volume']].copy()\n",
        "    df.index.name = 'Date'\n",
        "    df['Ticker'] = ticker\n",
        "    return df\n",
        "\n",
        "def compute_rsi(series: pd.Series, period: int = 14) -> pd.Series:\n",
        "    series = series.astype(float)\n",
        "    delta = series.diff()\n",
        "    up = delta.clip(lower=0.0)\n",
        "    down = (-delta).clip(lower=0.0)\n",
        "    roll_up = up.ewm(alpha=1/period, adjust=False).mean()\n",
        "    roll_down = down.ewm(alpha=1/period, adjust=False).mean()\n",
        "    rs = roll_up / (roll_down + 1e-8)\n",
        "    return 100.0 - (100.0 / (1.0 + rs))\n",
        "\n",
        "def make_features(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Compute pct-change, rolling mean/vol(10), RSI(14) on adjusted close if present.\n",
        "    Add next-day direction target y (1 if up, else 0). Drops NaNs and sorts index.\n",
        "    \"\"\"\n",
        "    out = df.copy()\n",
        "    out = out.sort_index()\n",
        "\n",
        "    # Prefer adjusted. If converter already replaced Close with adjusted, this is a no-op.\n",
        "    price = out['AdjClose'] if 'AdjClose' in out.columns else out['Close']\n",
        "\n",
        "    out['pct_change']   = price.pct_change()\n",
        "    out['roll_mean_10'] = out['pct_change'].rolling(window=10, min_periods=10).mean()\n",
        "    out['roll_vol_10']  = out['pct_change'].rolling(window=10, min_periods=10).std()\n",
        "    out['rsi_14']       = compute_rsi(price, 14)\n",
        "\n",
        "    # Target uses forward return on the same price series\n",
        "    out['ret_fwd_1'] = price.pct_change().shift(-1)\n",
        "    out['y'] = (out['ret_fwd_1'] > 0).astype(int)\n",
        "\n",
        "    # Final cleanup: drop rows with any NaNs in features/target\n",
        "    out = out.dropna(subset=['pct_change', 'roll_mean_10', 'roll_vol_10', 'rsi_14', 'ret_fwd_1', 'y'])\n",
        "    return out\n",
        "\n",
        "def load_all_tickers(tickers, start, end):\n",
        "    frames = []\n",
        "    for t in tickers:\n",
        "        raw = load_ohlcv_yf(t, start, end)\n",
        "        feats = make_features(raw)\n",
        "        frames.append(feats)\n",
        "    return pd.concat(frames).sort_index()\n",
        "\n",
        "# TODO: Optionally visualize a sample\n",
        "print('Data/feature functions defined. Fill TODOs as needed.')\n"
      ],
      "id": "N8hKKGhFMcZB"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CSV→Parquet converter"
      ],
      "metadata": {
        "id": "RwhtxRzKprt3"
      },
      "id": "RwhtxRzKprt3"
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# --- CSV → Parquet (one-time) ---\n",
        "import glob\n",
        "\n",
        "# Your paths\n",
        "OUTPUT_DIR = './outputs'\n",
        "DATA_IN_DIRS = [\n",
        "    '/content/drive/MyDrive/stock_market_data/nasdaq/csv',\n",
        "    '/content/drive/MyDrive/stock_market_data/nyse/csv',\n",
        "]\n",
        "#PARQUET_DIR = '/content/data_parquet'\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "os.makedirs(PARQUET_DIR, exist_ok=True)\n",
        "\n",
        "# Map your CSV headers to canonical names we use elsewhere\n",
        "CSV_COLS = {\n",
        "    'Date': 'Date',\n",
        "    'Open': 'Open',\n",
        "    'High': 'High',\n",
        "    'Low': 'Low',\n",
        "    'Close': 'Close',                 # raw close (we'll replace with Adjusted Close for features)\n",
        "    'Volume': 'Volume',\n",
        "    'Adjusted Close': 'AdjClose',\n",
        "}\n",
        "\n",
        "def guess_ticker_from_path(p):\n",
        "    import os\n",
        "    return os.path.splitext(os.path.basename(p))[0].upper()\n",
        "\n",
        "def load_csv_one(p):\n",
        "    # Read permissively (handles odd rows/column orders)\n",
        "    df = pd.read_csv(\n",
        "        p,\n",
        "        engine='python',           # robust parser\n",
        "        on_bad_lines='skip'        # skip corrupted rows\n",
        "    )\n",
        "\n",
        "    # Normalize column names (lower -> canonical)\n",
        "    orig_cols = df.columns.tolist()\n",
        "    lc_map = {c.lower().strip(): c for c in df.columns}\n",
        "\n",
        "    # Find variants\n",
        "    date_col = lc_map.get('date')\n",
        "    open_col = lc_map.get('open')\n",
        "    high_col = lc_map.get('high')\n",
        "    low_col  = lc_map.get('low')\n",
        "    close_col= lc_map.get('close')\n",
        "    vol_col  = lc_map.get('volume')\n",
        "    adj_col  = lc_map.get('adjusted close') or lc_map.get('adj close') or lc_map.get('adjclose')\n",
        "\n",
        "    # Hard fail if no Date\n",
        "    if not date_col:\n",
        "        raise ValueError(f'No Date column in {p} (had: {orig_cols})')\n",
        "\n",
        "    # Build a minimal frame with whatever we found\n",
        "    cols = {}\n",
        "    cols['Date']   = date_col\n",
        "    if open_col:  cols['Open']  = open_col\n",
        "    if high_col:  cols['High']  = high_col\n",
        "    if low_col:   cols['Low']   = low_col\n",
        "    if close_col: cols['Close'] = close_col\n",
        "    if vol_col:   cols['Volume']= vol_col\n",
        "    if adj_col:   cols['AdjClose'] = adj_col\n",
        "\n",
        "    df = df.rename(columns={v:k for k,v in cols.items()})[list(cols.keys())]\n",
        "\n",
        "    # Parse day-first dates like 15-12-2010\n",
        "    df['Date'] = pd.to_datetime(df['Date'], dayfirst=True, errors='coerce')\n",
        "    df = df.dropna(subset=['Date']).sort_values('Date').set_index('Date')\n",
        "\n",
        "    # Prefer adjusted close for features; fall back to Close\n",
        "    if 'AdjClose' in df.columns:\n",
        "        df['Close_raw'] = df.get('Close', df['AdjClose'])\n",
        "        df['Close'] = df['AdjClose']\n",
        "    else:\n",
        "        # No adjusted close available; use raw Close\n",
        "        if 'Close' not in df.columns:\n",
        "            raise ValueError(f'No Close/AdjClose in {p} (had: {orig_cols})')\n",
        "        df['Close_raw'] = df['Close']\n",
        "\n",
        "    # Coerce numerics (strip stray chars like $ or commas)\n",
        "    for c in ['Open','High','Low','Close','Volume','AdjClose','Close_raw']:\n",
        "        if c in df.columns:\n",
        "            df[c] = (df[c].astype(str)\n",
        "                           .str.replace(r'[^0-9\\.\\-eE]', '', regex=True)\n",
        "                           .replace({'': None}))\n",
        "            df[c] = pd.to_numeric(df[c], errors='coerce')\n",
        "\n",
        "    # Basic sanity: drop rows missing Close or Volume\n",
        "    need = ['Close']\n",
        "    if 'Volume' in df.columns: need.append('Volume')\n",
        "    df = df.dropna(subset=need)\n",
        "\n",
        "    df['Ticker'] = guess_ticker_from_path(p)\n",
        "    return df\n",
        "\n",
        "def iter_csv_files():\n",
        "    for d in DATA_IN_DIRS:\n",
        "        for p in glob.glob(os.path.join(d, '*.csv')):\n",
        "            yield p\n",
        "\n",
        "def build_parquet_once():\n",
        "    converted, skipped = 0, []\n",
        "    for p in iter_csv_files():\n",
        "        tkr = guess_ticker_from_path(p)\n",
        "        outp = os.path.join(PARQUET_DIR, f'{tkr}.parquet')\n",
        "        if os.path.exists(outp):\n",
        "            continue\n",
        "        try:\n",
        "            df = load_csv_one(p)\n",
        "            feats = make_features(df)  # uses adjusted Close internally now\n",
        "            feats = feats.drop(columns=[c for c in ['AdjClose','Close_raw'] if c in feats.columns], errors='ignore')\n",
        "            feats.to_parquet(outp)\n",
        "            converted += 1\n",
        "            if converted % 50 == 0:\n",
        "                print(f'Converted {converted} tickers...')\n",
        "        except Exception as e:\n",
        "            skipped.append((tkr, str(e)))\n",
        "            if len(skipped) <= 5:\n",
        "                print(f'[WARN] Skipping {tkr}: {e}')\n",
        "    print(f'Parquet conversion complete. Total tickers converted: {converted}. Skipped: {len(skipped)}')\n",
        "    if skipped:\n",
        "        import json\n",
        "        with open(os.path.join(OUTPUT_DIR, 'skipped_files.json'), 'w') as f:\n",
        "            json.dump(skipped, f, indent=2)\n",
        "        print('Wrote outputs/skipped_files.json')\n",
        "\n",
        "build_parquet_once()\n",
        "'''\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "iBt_58EIqn1U",
        "outputId": "ce69f86b-a097-47d1-d8f5-12e2a1615749"
      },
      "id": "iBt_58EIqn1U",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "incomplete input (ipython-input-265338909.py, line 128)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-265338909.py\"\u001b[0;36m, line \u001b[0;32m128\u001b[0m\n\u001b[0;31m    '''\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parquet Loader"
      ],
      "metadata": {
        "id": "6lra6mHfrkVu"
      },
      "id": "6lra6mHfrkVu"
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Parquet path index (no big concat) ---\n",
        "import os, glob\n",
        "import pandas as pd\n",
        "\n",
        "# Gather all ticker parquet files\n",
        "paths = sorted(glob.glob(os.path.join(PARQUET_DIR, \"*.parquet\")))\n",
        "S = PARAMS['split']\n",
        "L = int(PARAMS['lookback'])\n",
        "\n",
        "# Count rows for a single file within a date window, using a light read when possible\n",
        "def count_rows_in_range(p, start, end):\n",
        "    any_col = (PARAMS.get('features', []) + ['y', 'Close', 'Open'])[0]\n",
        "    try:\n",
        "        df = pd.read_parquet(p, columns=[any_col])\n",
        "    except Exception:\n",
        "        df = pd.read_parquet(p)\n",
        "\n",
        "    # Ensure DatetimeIndex\n",
        "    if not isinstance(df.index, pd.DatetimeIndex):\n",
        "        if 'Date' in df.columns:\n",
        "            df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
        "            df = df.dropna(subset=['Date']).set_index('Date')\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "    df = df.sort_index()\n",
        "    df = df.loc[(df.index >= pd.to_datetime(start)) & (df.index <= pd.to_datetime(end))]\n",
        "    return len(df)\n",
        "\n",
        "# Keep only tickers that have enough rows in ALL splits\n",
        "keep_paths = []\n",
        "for p in paths:\n",
        "    n_tr = count_rows_in_range(p, S['train_start'], S['train_end'])\n",
        "    n_va = count_rows_in_range(p, S['val_start'],   S['val_end'])\n",
        "    n_te = count_rows_in_range(p, S['test_start'],  S['test_end'])\n",
        "    if (n_tr >= L + 2) and (n_va >= L + 2) and (n_te >= L + 2):\n",
        "        keep_paths.append(p)\n",
        "\n",
        "# Build per-split lists from the intersection (defensive re-check)\n",
        "def filter_paths_by_range(ps, start, end):\n",
        "    out = []\n",
        "    for p in ps:\n",
        "        if count_rows_in_range(p, start, end) >= L + 2:\n",
        "            out.append(p)\n",
        "    return out\n",
        "\n",
        "train_paths = filter_paths_by_range(keep_paths, S['train_start'], S['train_end'])\n",
        "val_paths   = filter_paths_by_range(keep_paths, S['val_start'],   S['val_end'])\n",
        "test_paths  = filter_paths_by_range(keep_paths, S['test_start'],  S['test_end'])\n",
        "\n",
        "print(\"Parquet files scanned:\", len(paths))\n",
        "print(\"tickers kept (all splits ≥ lookback+2):\", len(keep_paths))\n",
        "print(f\"train_paths: {len(train_paths)}, val_paths: {len(val_paths)}, test_paths: {len(test_paths)}\")\n"
      ],
      "metadata": {
        "id": "Mv0EewBFrq04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "183b0032-fd58-43f4-8b6d-e97c008ad80d"
      },
      "id": "Mv0EewBFrq04",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tickers kept: 2406\n",
            "train_paths: 2406, val_paths: 2406, test_paths: 2406\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Single-split indexing + streaming loader\n",
        "# We use PARAMS['split'] (train/val/test date ranges) and Bootstrap A to build:\n",
        "#   - train_paths / val_paths / test_paths  (each file has ≥ lookback+2 rows in that window)\n",
        "# Data are streamed per-ticker via TickerIterableDataset.\n",
        "# Feature normalization uses feature_norm.csv loaded in Bootstrap A; we normalize on-the-fly."
      ],
      "metadata": {
        "id": "uJzimdv6fSXv"
      },
      "id": "uJzimdv6fSXv"
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Build per-split ticker file lists (tiny memory) ---\n",
        "import os, glob, pandas as pd\n",
        "\n",
        "paths = sorted(glob.glob(os.path.join(PARQUET_DIR, \"*.parquet\")))\n",
        "S = PARAMS[\"split\"]\n",
        "L = int(PARAMS[\"lookback\"])\n",
        "\n",
        "def count_rows_in_range(p, start, end):\n",
        "    \"\"\"Lightweight row count for a single file within [start, end], avoiding big reads.\"\"\"\n",
        "    any_col = (PARAMS.get(\"features\", []) + [\"y\", \"Close\", \"Open\"])[0]\n",
        "    try:\n",
        "        df = pd.read_parquet(p, columns=[any_col])\n",
        "    except Exception:\n",
        "        df = pd.read_parquet(p)\n",
        "\n",
        "    # Ensure DatetimeIndex\n",
        "    if not isinstance(df.index, pd.DatetimeIndex):\n",
        "        if \"Date\" in df.columns:\n",
        "            df[\"Date\"] = pd.to_datetime(df[\"Date\"], errors=\"coerce\")\n",
        "            df = df.dropna(subset=[\"Date\"]).set_index(\"Date\")\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "    df = df.sort_index()\n",
        "    df = df.loc[(df.index >= pd.to_datetime(start)) & (df.index <= pd.to_datetime(end))]\n",
        "    return len(df)\n",
        "\n",
        "# Keep only tickers with enough rows in ALL splits\n",
        "keep_paths = []\n",
        "for p in paths:\n",
        "    n_tr = count_rows_in_range(p, S[\"train_start\"], S[\"train_end\"])\n",
        "    n_va = count_rows_in_range(p, S[\"val_start\"],   S[\"val_end\"])\n",
        "    n_te = count_rows_in_range(p, S[\"test_start\"],  S[\"test_end\"])\n",
        "    if (n_tr >= L + 2) and (n_va >= L + 2) and (n_te >= L + 2):\n",
        "        keep_paths.append(p)\n",
        "\n",
        "def filter_paths(ps, start, end):\n",
        "    out = []\n",
        "    for p in ps:\n",
        "        if count_rows_in_range(p, start, end) >= L + 2:\n",
        "            out.append(p)\n",
        "    return out\n",
        "\n",
        "train_paths = filter_paths(keep_paths, S[\"train_start\"], S[\"train_end\"])\n",
        "val_paths   = filter_paths(keep_paths, S[\"val_start\"],   S[\"val_end\"])\n",
        "test_paths  = filter_paths(keep_paths, S[\"test_start\"],  S[\"test_end\"])\n",
        "\n",
        "print(f\"Kept: {len(keep_paths)} tickers | \"\n",
        "      f\"train={len(train_paths)} val={len(val_paths)} test={len(test_paths)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLJFgmKr4hVv",
        "outputId": "5ff82554-6b2d-4186-9f8e-70074f26d50b"
      },
      "id": "MLJFgmKr4hVv",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kept: 2406 tickers | train=2406 val=2406 test=2406\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Streaming dataset: reads per-ticker on the fly, applies normalization, yields windows ---\n",
        "import os, pandas as pd, numpy as np, torch\n",
        "from torch.utils.data import IterableDataset, DataLoader\n",
        "\n",
        "feat_cols = PARAMS[\"features\"]\n",
        "lb = int(PARAMS[\"lookback\"])\n",
        "\n",
        "class TickerIterableDataset(IterableDataset):\n",
        "    def __init__(self, parquet_paths, split_start, split_end):\n",
        "        self.paths = parquet_paths\n",
        "        self.start = pd.to_datetime(split_start)\n",
        "        self.end   = pd.to_datetime(split_end)\n",
        "\n",
        "    def __iter__(self):\n",
        "        for p in self.paths:\n",
        "            # read only what we need; keep memory tiny\n",
        "            df = pd.read_parquet(p, columns=feat_cols + [\"y\"]).sort_index()\n",
        "            df = df.loc[(df.index >= self.start) & (df.index <= self.end)]\n",
        "            if len(df) < lb + 2:\n",
        "                continue\n",
        "            df = normalize_inplace(df)\n",
        "            X = df[feat_cols].to_numpy(dtype=np.float32, copy=False)\n",
        "            y = df[\"y\"].to_numpy(dtype=np.int64,   copy=False)\n",
        "\n",
        "            # yield sliding windows without prebuilding a giant array\n",
        "            for i in range(lb, len(df)):\n",
        "                yield X[i - lb:i], y[i]\n",
        "\n",
        "# DataLoaders (keep workers=0 in Colab to avoid extra RAM)\n",
        "S = PARAMS['split']\n",
        "BATCH = min(1024, int(PARAMS.get(\"batch_size\", 128)))\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    TickerIterableDataset(train_paths, S[\"train_start\"], S[\"train_end\"]),\n",
        "    batch_size=BATCH, shuffle=False, num_workers=0, pin_memory=True,\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    TickerIterableDataset(val_paths, S[\"val_start\"], S[\"val_end\"]),\n",
        "    batch_size=BATCH, shuffle=False, num_workers=0, pin_memory=True,\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    TickerIterableDataset(test_paths, S[\"test_start\"], S[\"test_end\"]),\n",
        "    batch_size=BATCH, shuffle=False, num_workers=0, pin_memory=True,\n",
        ")\n",
        "\n",
        "print(f\"Streaming loaders ready. batch={BATCH} | tickers train/val/test = \"\n",
        "      f\"{len(train_paths)}/{len(val_paths)}/{len(test_paths)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fkNqCwKp6WSO",
        "outputId": "95d08df5-3790-4163-deaa-87c4726ece00"
      },
      "id": "fkNqCwKp6WSO",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Streaming loaders ready. batch=1024 | tickers train/val/test = 2406/2406/2406\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_Wih2_bMcZD"
      },
      "source": [
        "## Model (Option A/B/C stubs)"
      ],
      "id": "m_Wih2_bMcZD"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "GXvo5I5OMcZE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ee669eb-f016-4e0e-a1cc-9bd7dca16977"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model stubs ready (A/B in PyTorch, C via sklearn).\n"
          ]
        }
      ],
      "source": [
        "#       *** DEFINE MODEL ***\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class ConvTransformer(nn.Module):\n",
        "    def __init__(self, in_feats, d_model=64, nhead=8, dim_ff=128, num_layers=2, dropout=0.1, num_classes=2):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Conv1d(in_channels=in_feats, out_channels=d_model, kernel_size=3, padding=1)\n",
        "        enc_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model, nhead=nhead, dim_feedforward=dim_ff, dropout=dropout, batch_first=True\n",
        "        )\n",
        "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n",
        "        self.fc = nn.Linear(d_model, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, T, F)\n",
        "        x = x.transpose(1, 2)          # (B, F, T)\n",
        "        x = torch.relu(self.proj(x))   # (B, d_model, T)\n",
        "        x = x.transpose(1, 2)          # (B, T, d_model)\n",
        "        x = self.encoder(x)            # (B, T, d_model)\n",
        "        x = x.mean(dim=1)              # GAP over time\n",
        "        return self.fc(x)\n",
        "\n",
        "class CNN_GRU(nn.Module):\n",
        "    \"\"\"Option B: 1D-CNN + small GRU head → logits(2).\"\"\"\n",
        "    def __init__(self, in_feats, hidden=32, num_classes=2):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv1d(in_channels=in_feats, out_channels=hidden, kernel_size=3, padding=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.gru = nn.GRU(input_size=hidden, hidden_size=hidden, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden, num_classes)\n",
        "    def forward(self, x):\n",
        "        x = x.transpose(1, 2)       # (B, F, T)\n",
        "        x = self.conv(x)            # (B, H, T)\n",
        "        x = x.transpose(1, 2)       # (B, T, H)\n",
        "        _, h = self.gru(x)          # h: (1, B, H)\n",
        "        h = h.squeeze(0)\n",
        "        return self.fc(h)\n",
        "\n",
        "def build_model(option: str, in_feats: int):\n",
        "    if option == 'A':\n",
        "        return ConvTransformer(in_feats)\n",
        "    raise ValueError('Only Option A is enabled right now')\n",
        "\n",
        "print('Model stubs ready (A/B in PyTorch, C via sklearn).')\n"
      ],
      "id": "GXvo5I5OMcZE"
    },
    {
      "cell_type": "code",
      "source": [
        "# --- BOOTSTRAP B: attach checkpoint, set session settings, and quick smoke test ---\n",
        "import os, torch\n",
        "\n",
        "# 0) Device\n",
        "device = (\n",
        "    \"cuda\" if torch.cuda.is_available()\n",
        "    else (\"mps\" if getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available() else \"cpu\")\n",
        ")\n",
        "\n",
        "# 0b) Optional local overrides (define LOCAL_OVERRIDES in any earlier cell if you want to override locked params)\n",
        "if \"LOCAL_OVERRIDES\" in globals() and isinstance(LOCAL_OVERRIDES, dict) and LOCAL_OVERRIDES:\n",
        "    PARAMS.update(LOCAL_OVERRIDES)\n",
        "\n",
        "# 1) Build model (requires build_model + PARAMS['features'])\n",
        "assert \"build_model\" in globals(), \"Define build_model(...) before running Bootstrap B.\"\n",
        "num_features = len(PARAMS[\"features\"])\n",
        "try:\n",
        "    model = build_model(PARAMS.get(\"model_option\"), in_feats=num_features)\n",
        "except TypeError:\n",
        "    # Fallback if your build_model signature is simpler\n",
        "    model = build_model(num_features)\n",
        "model = model.to(device)\n",
        "\n",
        "# 2) Load checkpoint (robust to different save formats)\n",
        "ckpt_path = os.path.join(OUTPUT_DIR, \"best_model.pt\")\n",
        "optimizer = None\n",
        "CKPT_META = {}\n",
        "if os.path.exists(ckpt_path):\n",
        "    state = torch.load(ckpt_path, map_location=device)\n",
        "    if isinstance(state, dict):\n",
        "        if \"model\" in state:           # common: {\"model\": state_dict, \"optimizer\": ..., \"epoch\": ...}\n",
        "            state_dict = state[\"model\"]\n",
        "        elif \"state_dict\" in state:     # some trainers\n",
        "            state_dict = state[\"state_dict\"]\n",
        "        else:                           # assume it's already a state_dict-like mapping\n",
        "            state_dict = state\n",
        "    else:\n",
        "        state_dict = state\n",
        "\n",
        "    missing, unexpected = model.load_state_dict(state_dict, strict=False)\n",
        "    model.eval()\n",
        "\n",
        "    # Optional: resume optimizer if you have make_optimizer(...)\n",
        "    if isinstance(state, dict) and \"optimizer\" in state and \"make_optimizer\" in globals():\n",
        "        try:\n",
        "            optimizer = make_optimizer(model, PARAMS)\n",
        "            optimizer.load_state_dict(state[\"optimizer\"])\n",
        "        except Exception:\n",
        "            optimizer = None  # don't fail Bootstrap if optimizer schema changed\n",
        "\n",
        "    CKPT_META = {\n",
        "        \"epoch\": (state.get(\"epoch\") if isinstance(state, dict) else None),\n",
        "        \"best_metric\": (state.get(\"best_metric\") if isinstance(state, dict) else None),\n",
        "        \"missing_keys\": missing, \"unexpected_keys\": unexpected,\n",
        "    }\n",
        "    print(f\"Loaded checkpoint: {ckpt_path}\")\n",
        "else:\n",
        "    model.eval()\n",
        "    print(\"No checkpoint found — train once; this cell will auto-load on next run.\")\n",
        "\n",
        "# 3) Session settings (temperature, margin, mode)\n",
        "T_best = float(PARAMS.get(\"calibration_T\", 1.0))\n",
        "margin = float(PARAMS.get(\"margin\", 0.15))\n",
        "mode   = str(PARAMS.get(\"portfolio_mode\", \"long_short\"))\n",
        "print(f\"Session settings → T={T_best:.3f}, margin={margin}, mode={mode}, device={device}\")\n",
        "\n",
        "# 4) Lightweight inference helper (applies temperature; works for binary or multi-class)\n",
        "@torch.no_grad()\n",
        "def model_probs(batch_windows, T=None):\n",
        "    \"\"\"batch_windows: numpy/torch with shape [B, L, F]. Returns numpy probs.\"\"\"\n",
        "    import numpy as np\n",
        "    x = torch.as_tensor(batch_windows, dtype=torch.float32, device=device)\n",
        "    logits = model(x)\n",
        "    t = float(T if T is not None else T_best)\n",
        "    if logits.ndim == 1 or logits.shape[-1] == 1:\n",
        "        z = logits.squeeze(-1) / t\n",
        "        return torch.sigmoid(z).detach().cpu().numpy()\n",
        "    return torch.softmax(logits / t, dim=-1).detach().cpu().numpy()\n",
        "\n",
        "# 5) Tiny smoke test (toggle on/off)\n",
        "RUN_SMOKE = True\n",
        "if RUN_SMOKE:\n",
        "    import numpy as np, pandas as pd\n",
        "    # gather candidate files from Bootstrap A\n",
        "    cands = []\n",
        "    for _nm in (\"train_paths\", \"val_paths\", \"test_paths\"):\n",
        "        if _nm in globals():\n",
        "            cands += globals()[_nm]\n",
        "    if cands:\n",
        "        try:\n",
        "            sample = cands[0]\n",
        "            need_cols = list(set(PARAMS[\"features\"] + [\"y\"]))\n",
        "            df = pd.read_parquet(sample, columns=need_cols).sort_index().tail(200)\n",
        "            if \"normalize_inplace\" in globals():\n",
        "                df = normalize_inplace(df)\n",
        "            L = int(PARAMS[\"lookback\"])\n",
        "            if len(df) > L + 1:\n",
        "                X = df[PARAMS[\"features\"]].to_numpy(np.float32)\n",
        "                X = np.stack([X[i-L:i] for i in range(L, len(X))], axis=0)\n",
        "                p = model_probs(X)[:3]\n",
        "                print(\"Smoke OK:\", X.shape, \"first probs:\", np.round(p, 3))\n",
        "            else:\n",
        "                print(\"Smoke skipped: not enough rows in sample for lookback =\", L)\n",
        "        except Exception as e:\n",
        "            print(\"Smoke failed (non-fatal):\", type(e).__name__, str(e)[:200])\n",
        "    else:\n",
        "        print(\"Smoke skipped: no *_paths available (run Bootstrap A first).\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rL1RzlZlRqBl",
        "outputId": "742ebce1-c2a8-4d94-89f2-58c2e7f113f9"
      },
      "id": "rL1RzlZlRqBl",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded checkpoint: /content/drive/MyDrive/Colab Notebooks/outputs/best_model.pt\n",
            "Session settings → T=0.899, margin=0.02, mode=long_only, device=cuda\n",
            "Smoke OK: (136, 64, 4) first probs: [[0.513 0.487]\n",
            " [0.513 0.487]\n",
            " [0.513 0.487]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Train: stabilized (no AMP), lower LR, grad clipping ---\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "model = build_model(PARAMS['model_option'], in_feats=len(PARAMS['features'])).to(device)\n",
        "crit  = nn.CrossEntropyLoss()\n",
        "opt   = torch.optim.Adam(model.parameters(), lr=3e-4, weight_decay=1e-4)  # lower LR + small WD\n",
        "\n",
        "def run_val(loader):\n",
        "    model.eval(); correct=total=0\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in loader:\n",
        "            xb = xb.to(device); yb = yb.to(device)\n",
        "            logits = model(xb)\n",
        "            pred = logits.argmax(1)\n",
        "            correct += (pred==yb).sum().item(); total += yb.numel()\n",
        "    return correct / max(total,1)\n",
        "\n",
        "best_acc = -1.0\n",
        "best_path = os.path.join(OUTPUT_DIR, 'best_model.pt')\n",
        "\n",
        "for ep in range(1, 3):  # start with 2 epochs to verify stability/speed\n",
        "    model.train()\n",
        "    running = 0.0; steps = 0\n",
        "    for xb, yb in train_loader:\n",
        "        xb = xb.to(device); yb = yb.to(device)\n",
        "        opt.zero_grad(set_to_none=True)\n",
        "        logits = model(xb)\n",
        "        loss = crit(logits, yb)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # grad clip\n",
        "        opt.step()\n",
        "        running += float(loss.item()); steps += 1\n",
        "        # (optional) speed probe:\n",
        "        # if steps % 5000 == 0: break\n",
        "\n",
        "    val_acc = run_val(val_loader)\n",
        "    print(f\"Epoch {ep}/2 | train_loss ~ {running/max(steps,1):.4f} | val_acc={val_acc:.3f}\")\n",
        "\n",
        "    if val_acc > best_acc:\n",
        "        best_acc = val_acc\n",
        "        torch.save(model.state_dict(), best_path)\n",
        "        print(f\"  ↳ saved {best_path} (best so far)\")\n",
        "\n",
        "print(f\"Best val_acc: {best_acc:.3f}\")\n"
      ],
      "metadata": {
        "id": "59jEmFp9F10F"
      },
      "id": "59jEmFp9F10F",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYapWNS5McZF"
      },
      "source": [
        "## Train (per split)"
      ],
      "id": "VYapWNS5McZF"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0yeX_KdMcZG"
      },
      "source": [
        "## Inference & backtest"
      ],
      "id": "Z0yeX_KdMcZG"
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Eval dataset + inference + backtest (uses next-day returns) ---\n",
        "import os, json, numpy as np, pandas as pd, torch\n",
        "from torch.utils.data import IterableDataset, DataLoader\n",
        "\n",
        "# 1) Dataset that yields (window, label, next-day return)\n",
        "class TickerEvalIterableDataset(IterableDataset):\n",
        "    def __init__(self, parquet_paths, start, end, features, lookback):\n",
        "        self.paths = parquet_paths\n",
        "        self.start = pd.to_datetime(start)\n",
        "        self.end   = pd.to_datetime(end)\n",
        "        self.features = features\n",
        "        self.lookback = int(lookback)\n",
        "\n",
        "    def __iter__(self):\n",
        "        for p in self.paths:\n",
        "            # We require 'ret_fwd_1' for PnL. If it's missing, raise a clear error.\n",
        "            need_cols = self.features + ['y', 'ret_fwd_1']\n",
        "            try:\n",
        "                df = pd.read_parquet(p, columns=need_cols).sort_index()\n",
        "            except Exception as e:\n",
        "                raise RuntimeError(f\"'ret_fwd_1' not found in {p}. \"\n",
        "                                   f\"Make sure your feature build saved that column.\") from e\n",
        "\n",
        "            df = df.loc[(df.index >= self.start) & (df.index <= self.end)]\n",
        "            if len(df) < self.lookback + 2:\n",
        "                continue\n",
        "\n",
        "            # use the same on-the-fly normalization as training\n",
        "            df = normalize_inplace(df)\n",
        "\n",
        "            X = df[self.features].to_numpy(dtype=np.float32, copy=False)\n",
        "            y = df['y'].to_numpy(dtype=np.int64,   copy=False)\n",
        "            r = df['ret_fwd_1'].to_numpy(dtype=np.float32, copy=False)\n",
        "\n",
        "            for i in range(self.lookback, len(df)):\n",
        "                yield X[i-self.lookback:i], y[i], r[i]\n",
        "\n",
        "# 2) Small helpers\n",
        "def loader_from_eval(paths, start, end, batch_size=1024):\n",
        "    ds = TickerEvalIterableDataset(paths, start, end, PARAMS['features'], PARAMS['lookback'])\n",
        "    return DataLoader(ds, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
        "\n",
        "@torch.no_grad()\n",
        "def infer_probs_and_returns(model, loader, device):\n",
        "    model.eval()\n",
        "    probs, rets, labels = [], [], []\n",
        "    for xb, yb, rb in loader:\n",
        "        xb = xb.to(device, non_blocking=True)\n",
        "        logits = model(xb)\n",
        "        p1 = torch.softmax(logits, dim=1)[:, 1].cpu().numpy()\n",
        "        probs.append(p1); rets.append(rb.numpy()); labels.append(yb.numpy())\n",
        "    if not probs:\n",
        "        return np.array([]), np.array([]), np.array([])\n",
        "    return np.concatenate(probs), np.concatenate(rets), np.concatenate(labels)\n",
        "\n",
        "def positions_from_probs(probs, th_long, th_short):\n",
        "    pos = np.zeros_like(probs, dtype=np.int8)\n",
        "    pos[probs >= th_long] = 1\n",
        "    pos[probs <= th_short] = -1\n",
        "    return pos\n",
        "\n",
        "def backtest_from_positions(returns, positions, cost_bps=5):\n",
        "    # position at t-1 applied to return at t (execute at next bar)\n",
        "    pos_shift = np.roll(positions.astype(int), 1); pos_shift[0] = 0\n",
        "    # transaction costs when position changes\n",
        "    trades = (pos_shift[1:] != pos_shift[:-1]).astype(int)\n",
        "    costs = np.zeros_like(returns, dtype=np.float32)\n",
        "    costs[1:] = trades * (cost_bps / 1e4)\n",
        "\n",
        "    daily_pnl = pos_shift * returns - costs\n",
        "    equity = (1.0 + daily_pnl).cumprod()\n",
        "    return equity, trades, daily_pnl\n",
        "\n",
        "def compute_metrics(equity, daily_pnl):\n",
        "    if len(equity) == 0:\n",
        "        return {\"CAGR\": 0.0, \"MaxDD\": 0.0, \"Sharpe\": 0.0}\n",
        "    T = len(equity)\n",
        "    years = T / 252.0\n",
        "    cagr  = (equity[-1] ** (1/years) - 1) if years > 0 and equity[-1] > 0 else 0.0\n",
        "    peaks = np.maximum.accumulate(equity)\n",
        "    maxdd = float((equity/peaks - 1).min())\n",
        "    sharpe = float((daily_pnl.mean() / (daily_pnl.std() + 1e-8)) * np.sqrt(252))\n",
        "    return {\"CAGR\": float(cagr), \"MaxDD\": maxdd, \"Sharpe\": sharpe}\n",
        "\n",
        "# 3) Build loaders for val/test\n",
        "val_eval_loader  = loader_from_eval(val_paths,  PARAMS['split']['val_start'],  PARAMS['split']['val_end'],  batch_size=2048)\n",
        "test_eval_loader = loader_from_eval(test_paths, PARAMS['split']['test_start'], PARAMS['split']['test_end'], batch_size=2048)\n",
        "\n",
        "# 4) Load best checkpoint\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = build_model(PARAMS['model_option'], in_feats=len(PARAMS['features'])).to(device)\n",
        "state = torch.load(os.path.join(OUTPUT_DIR, 'best_model.pt'), map_location=device)\n",
        "sd = state if isinstance(state, dict) and 'model' not in state else state.get('model', state)\n",
        "model.load_state_dict(sd)\n",
        "\n",
        "# 5) Infer → positions → backtest → metrics\n",
        "probs_val,  rets_val,  y_val  = infer_probs_and_returns(model, val_eval_loader,  device)\n",
        "probs_test, rets_test, y_test = infer_probs_and_returns(model, test_eval_loader, device)\n",
        "print(\"Shapes | val:\", probs_val.shape, rets_val.shape, \"| test:\", probs_test.shape, rets_test.shape)\n",
        "\n",
        "# thresholds & costs (with safe defaults)\n",
        "th_long  = PARAMS.get('threshold_long', 0.55)\n",
        "th_short = PARAMS.get('threshold_short', 0.45)\n",
        "cost_bps = PARAMS.get('cost_bps', 5)\n",
        "\n",
        "pos_val  = positions_from_probs(probs_val,  th_long, th_short)\n",
        "pos_test = positions_from_probs(probs_test, th_long, th_short)\n",
        "\n",
        "eq_val,  trades_val,  pnl_val  = backtest_from_positions(rets_val,  pos_val,  cost_bps=cost_bps)\n",
        "eq_test, trades_test, pnl_test = backtest_from_positions(rets_test, pos_test, cost_bps=cost_bps)\n",
        "\n",
        "m_val  = compute_metrics(eq_val,  pnl_val)\n",
        "m_test = compute_metrics(eq_test, pnl_test)\n",
        "\n",
        "print(\"VAL  metrics:\", m_val,  \"| trades:\", int(trades_val.sum()))\n",
        "print(\"TEST metrics:\", m_test, \"| trades:\", int(trades_test.sum()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6iYzqIItclE",
        "outputId": "361cbbd5-e03b-41d4-e5c7-948b7f5a7dff"
      },
      "id": "b6iYzqIItclE",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapes | val: (452128,) (452128,) | test: (1624247,) (1624247,)\n",
            "VAL  metrics: {'CAGR': 0.0, 'MaxDD': -1.0, 'Sharpe': -0.06684386441090877} | trades: 1400\n",
            "TEST metrics: {'CAGR': 0.0, 'MaxDD': -1.0, 'Sharpe': -0.024597495151045513} | trades: 5236\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Evaluation (subset) — uses PARAMS['calibration_T'] and PARAMS['margin'] ---\n",
        "import os, json, math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# knobs driven by PARAMS (no hardcoding)\n",
        "SPLITS    = [\"val\", \"test\"]        # run both\n",
        "N_TICKERS = len(val_paths)#250                     # subset size\n",
        "T_BEST    = float(PARAMS.get(\"calibration_T\", 0.899))\n",
        "MARGIN    = float(PARAMS.get(\"margin\", 0.02))\n",
        "MODE      = str(PARAMS.get(\"portfolio_mode\", \"long_only\"))\n",
        "COST_BPS  = float(PARAMS.get(\"cost_bps\", 5))\n",
        "FEATS     = PARAMS[\"features\"]\n",
        "L         = int(PARAMS[\"lookback\"])\n",
        "S         = PARAMS[\"split\"]\n",
        "\n",
        "# helpers we expect from your canonical utilities (don’t redefine here):\n",
        "#   - sanitize_returns(...)\n",
        "#   - positions_from_probs(...)\n",
        "#   - compute_metrics(...)\n",
        "# and from Bootstrap B:\n",
        "#   - model_probs(...)\n",
        "# and from Bootstrap A:\n",
        "#   - normalize_inplace(...)\n",
        "\n",
        "paths_map = {\"train\": train_paths, \"val\": val_paths, \"test\": test_paths}\n",
        "\n",
        "def load_returns_and_probs(fp, start, end):\n",
        "    \"\"\"Load one ticker, normalize, make windows, get up-prob per step with T scaling.\"\"\"\n",
        "    # minimal columns with fallbacks\n",
        "    try:\n",
        "        df = pd.read_parquet(fp, columns=list(set(FEATS + [\"ret_fwd_1\", \"y\", \"Close\"]))).sort_index()\n",
        "    except Exception:\n",
        "        df = pd.read_parquet(fp).sort_index()\n",
        "\n",
        "    df = df.loc[(df.index >= pd.to_datetime(start)) & (df.index <= pd.to_datetime(end))]\n",
        "    if len(df) < L + 2:\n",
        "        return None\n",
        "\n",
        "    # derive forward return if missing\n",
        "    if \"ret_fwd_1\" not in df:\n",
        "        if \"y\" in df:\n",
        "            df[\"ret_fwd_1\"] = (df[\"y\"].astype(float) * 2 - 1).shift(-1).fillna(0.0)\n",
        "        elif \"Close\" in df:\n",
        "            cl = df[\"Close\"].astype(float)\n",
        "            df[\"ret_fwd_1\"] = np.log(cl.shift(-1) / cl).fillna(0.0)\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    # normalize features\n",
        "    df = normalize_inplace(df)\n",
        "    X = df[FEATS].to_numpy(np.float32, copy=False)\n",
        "    if len(X) <= L + 1:\n",
        "        return None\n",
        "\n",
        "    # build sliding windows (T-L steps), get probs with calibrated temperature\n",
        "    Xw = np.stack([X[i - L:i] for i in range(L, len(X))], axis=0)\n",
        "    p  = model_probs(Xw, T=T_BEST)\n",
        "    p  = p[:, 1] if (p.ndim == 2 and p.shape[1] == 2) else (p if p.ndim == 1 else p[:, -1])\n",
        "\n",
        "    # align returns to label steps\n",
        "    r = df[\"ret_fwd_1\"].to_numpy(np.float32, copy=False)[L:]\n",
        "    r = sanitize_returns(r, cap=0.30)\n",
        "    dates = df.index[L:]\n",
        "    return dates, r, p\n",
        "\n",
        "def portfolio_from_subset(split_name, n_limit):\n",
        "    files = paths_map[split_name][:n_limit]\n",
        "    if not files:\n",
        "        return None\n",
        "\n",
        "    start = S[f\"{split_name}_start\"]; end = S[f\"{split_name}_end\"]\n",
        "    eq_sums = {}   # date -> sum of per-ticker pnl\n",
        "    eq_cnts = {}   # date -> # active tickers\n",
        "\n",
        "    th_long  = 0.5 + MARGIN\n",
        "    th_short = 0.5 - MARGIN\n",
        "\n",
        "    for fp in files:\n",
        "        out = load_returns_and_probs(fp, start, end)\n",
        "        if out is None:\n",
        "            continue\n",
        "        dates, r, p = out\n",
        "\n",
        "        if MODE == \"long_only\":\n",
        "            pos = (p > th_long).astype(np.int8)          # 1 or 0\n",
        "        else:\n",
        "            pos = np.zeros_like(p, dtype=np.int8)\n",
        "            pos[p >= th_long] = 1\n",
        "            pos[p <= th_short] = -1\n",
        "\n",
        "        # entry/flip costs per ticker\n",
        "        pos_shift = np.roll(pos, 1); pos_shift[0] = 0\n",
        "        trades = (pos_shift[1:] != pos_shift[:-1]).astype(np.int8)\n",
        "        costs  = np.zeros_like(r, dtype=np.float32)\n",
        "        if len(costs) > 1:\n",
        "            costs[1:] = trades * (COST_BPS / 1e4)\n",
        "\n",
        "        pnl = (pos_shift * r) - costs\n",
        "\n",
        "        for d, x in zip(pd.to_datetime(dates).normalize(), pnl):\n",
        "            eq_sums[d] = eq_sums.get(d, 0.0) + float(x)\n",
        "            eq_cnts[d] = eq_cnts.get(d, 0) + 1\n",
        "\n",
        "    if not eq_sums:\n",
        "        return None\n",
        "\n",
        "    days = sorted(eq_sums.keys())\n",
        "    daily = pd.DataFrame({\n",
        "        \"date\": days,\n",
        "        \"ret_day\": [eq_sums[d] / max(eq_cnts[d], 1) for d in days],\n",
        "        \"n\": [eq_cnts[d] for d in days],\n",
        "    }).set_index(\"date\").sort_index()\n",
        "\n",
        "    # equity + metrics\n",
        "    rets = daily[\"ret_day\"].to_numpy(np.float64)\n",
        "    equity = np.cumprod(1.0 + np.clip(rets, -0.99, 10.0))\n",
        "    mets = compute_metrics(equity, rets)\n",
        "    return daily, equity, mets\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "run_summ = {\"T\": T_BEST, \"margin\": MARGIN, \"mode\": MODE, \"lookback\": L, \"features\": FEATS, \"subset\": N_TICKERS}\n",
        "\n",
        "for split in SPLITS:\n",
        "    res = portfolio_from_subset(split, N_TICKERS)\n",
        "    if res is None:\n",
        "        print(f\"{split.upper()} produced no data.\")\n",
        "        continue\n",
        "    daily, equity, mets = res\n",
        "\n",
        "    # save equity PNG + CSV\n",
        "    png = os.path.join(OUTPUT_DIR, f\"model_equity_{split}_subset.png\")\n",
        "    csv = os.path.join(OUTPUT_DIR, f\"model_daily_{split}_subset.csv\")\n",
        "    daily.to_csv(csv, index=True)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(equity)\n",
        "    plt.title(f\"Model Equity — {split.upper()} (subset={N_TICKERS}, T={T_BEST:.3f}, margin={MARGIN}, mode={MODE})\")\n",
        "    plt.xlabel(\"Days\"); plt.ylabel(\"Equity\"); plt.grid(True)\n",
        "    plt.savefig(png, dpi=150, bbox_inches=\"tight\"); plt.close()\n",
        "\n",
        "    run_summ[split] = {\"days\": int(daily.shape[0]), \"metrics\": mets, \"png\": png, \"csv\": csv}\n",
        "    print(f\"{split.upper()} metrics:\", mets)\n",
        "\n",
        "# Write run summary JSON\n",
        "run_json = os.path.join(OUTPUT_DIR, \"model_run_full.json\") #model_run_subset.json\n",
        "with open(run_json, \"w\") as f:\n",
        "    json.dump(run_summ, f, indent=2)\n",
        "print(\"Saved:\", run_json)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rLF8x4EqFcQK",
        "outputId": "c34a2fd9-a704-4a98-9c9d-3a82f21f638f"
      },
      "id": "rLF8x4EqFcQK",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VAL metrics: {'CAGR': 0.0032084100905311885, 'MaxDD': -0.002329084011930105, 'Sharpe': 0.894978358671953}\n",
            "TEST metrics: {'CAGR': 0.00576117112133212, 'MaxDD': -0.003667971007708104, 'Sharpe': 1.0546295781739405}\n",
            "Saved: /content/drive/MyDrive/Colab Notebooks/outputs/model_run_full.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify eval artifacts exist and look sane\n",
        "import os, glob, pandas as pd\n",
        "outs = sorted(glob.glob(os.path.join(OUTPUT_DIR, \"model_*_subset.*\")))\n",
        "print(\"Found:\", [os.path.basename(p) for p in outs])\n",
        "for p in outs:\n",
        "    try: print(os.path.basename(p), \"bytes:\", os.path.getsize(p))\n",
        "    except: pass\n",
        "\n",
        "for split in (\"val\",\"test\"):\n",
        "    csv = os.path.join(OUTPUT_DIR, f\"model_daily_{split}_subset.csv\")\n",
        "    if os.path.exists(csv):\n",
        "        print(\"\\n\", split.upper(), \"CSV tail:\")\n",
        "        display(pd.read_csv(csv).tail(3))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "id": "i9FduDNLGGLF",
        "outputId": "4f632e93-e717-4c43-eafb-bfaaa4dfe217"
      },
      "id": "i9FduDNLGGLF",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found: ['model_daily_test_subset.csv', 'model_daily_val_subset.csv', 'model_equity_test_subset.png', 'model_equity_val_subset.png', 'model_run_subset.json']\n",
            "model_daily_test_subset.csv bytes: 21159\n",
            "model_daily_val_subset.csv bytes: 6819\n",
            "model_equity_test_subset.png bytes: 52618\n",
            "model_equity_val_subset.png bytes: 66676\n",
            "model_run_subset.json bytes: 870\n",
            "\n",
            " VAL CSV tail:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "           date   ret_day    n\n",
              "185  2019-12-27 -0.000167  249\n",
              "186  2019-12-30  0.000133  250\n",
              "187  2019-12-31  0.000206  250"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d80bf783-2b6f-4b7b-8e7a-dbd46df77567\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>ret_day</th>\n",
              "      <th>n</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>185</th>\n",
              "      <td>2019-12-27</td>\n",
              "      <td>-0.000167</td>\n",
              "      <td>249</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>186</th>\n",
              "      <td>2019-12-30</td>\n",
              "      <td>0.000133</td>\n",
              "      <td>250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>187</th>\n",
              "      <td>2019-12-31</td>\n",
              "      <td>0.000206</td>\n",
              "      <td>250</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d80bf783-2b6f-4b7b-8e7a-dbd46df77567')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d80bf783-2b6f-4b7b-8e7a-dbd46df77567 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d80bf783-2b6f-4b7b-8e7a-dbd46df77567');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-5bd84f86-50f9-4a1e-afcd-37f058d96494\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5bd84f86-50f9-4a1e-afcd-37f058d96494')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-5bd84f86-50f9-4a1e-afcd-37f058d96494 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "repr_error": "0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " TEST CSV tail:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "           date  ret_day    n\n",
              "675  2022-12-07      0.0  246\n",
              "676  2022-12-08      0.0  246\n",
              "677  2022-12-09      0.0  237"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7f473e4b-51f4-4cfb-ab17-e67454650b81\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>ret_day</th>\n",
              "      <th>n</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>675</th>\n",
              "      <td>2022-12-07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>246</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>676</th>\n",
              "      <td>2022-12-08</td>\n",
              "      <td>0.0</td>\n",
              "      <td>246</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>677</th>\n",
              "      <td>2022-12-09</td>\n",
              "      <td>0.0</td>\n",
              "      <td>237</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7f473e4b-51f4-4cfb-ab17-e67454650b81')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7f473e4b-51f4-4cfb-ab17-e67454650b81 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7f473e4b-51f4-4cfb-ab17-e67454650b81');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-f75037a6-606c-4bcc-afe7-8a8647ef7fde\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f75037a6-606c-4bcc-afe7-8a8647ef7fde')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-f75037a6-606c-4bcc-afe7-8a8647ef7fde button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "repr_error": "0"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "daily.to_csv(os.path.join(OUTPUT_DIR, f\"model_daily_{split}_subset.csv\"), index=True)\n",
        "plt.savefig(os.path.join(OUTPUT_DIR, f\"model_equity_{split}_subset.png\"), dpi=150, bbox_inches=\"tight\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "MKcFGlyEGXj9",
        "outputId": "a600b62c-45cd-4c8d-8131-ee3177faae40"
      },
      "id": "MKcFGlyEGXj9",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Sanitize returns for backtest (handles percent-points & split spikes) ---\n",
        "import numpy as np\n",
        "\n",
        "def sanitize_returns(r, cap=0.30):\n",
        "    r = r.astype(np.float32).copy()\n",
        "\n",
        "    # Step A: if any values look like percent-points (|r| > 1), convert them to fractions\n",
        "    # Example: 99 -> 0.99, 3098 -> 30.98 (then we'll cap below)\n",
        "    pp_mask = np.abs(r) > 1.0\n",
        "    if pp_mask.any():\n",
        "        r[pp_mask] = r[pp_mask] / 100.0\n",
        "\n",
        "    # Step B: kill impossible negatives (bankruptcy in a day). Keep -0.99 as a hard floor.\n",
        "    r[r < -0.99] = -0.99\n",
        "\n",
        "    # Step C: cap absurd positives (splits/bad ticks). 30% cap is common; tune if needed.\n",
        "    r = np.clip(r, -cap, cap)\n",
        "\n",
        "    return r\n",
        "\n",
        "rets_val_s  = sanitize_returns(rets_val,  cap=0.30)\n",
        "rets_test_s = sanitize_returns(rets_test, cap=0.30)\n",
        "\n",
        "# Re-run backtest + metrics with sanitized returns\n",
        "eq_val,  trades_val,  pnl_val  = backtest_from_positions(rets_val_s,  pos_val,  cost_bps=cost_bps)\n",
        "eq_test, trades_test, pnl_test = backtest_from_positions(rets_test_s, pos_test, cost_bps=cost_bps)\n",
        "\n",
        "m_val  = compute_metrics(eq_val,  pnl_val)\n",
        "m_test = compute_metrics(eq_test, pnl_test)\n",
        "print(\"VAL  metrics (sanitized):\", m_val,  \"| trades:\", int(trades_val.sum()))\n",
        "print(\"TEST metrics (sanitized):\", m_test, \"| trades:\", int(trades_test.sum()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SfytNNRQ1Tk7",
        "outputId": "1052a436-8824-4d32-ef76-f5eae392bf99"
      },
      "id": "SfytNNRQ1Tk7",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VAL  metrics (sanitized): {'CAGR': -0.018236330647384835, 'MaxDD': -0.9999999999999987, 'Sharpe': 0.014450937035149873} | trades: 1400\n",
            "TEST metrics (sanitized): {'CAGR': -0.058430599608076506, 'MaxDD': -1.0, 'Sharpe': -0.16890677308891483} | trades: 5236\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Boundary-aware backtest (no cross-ticker carry; log-equity to avoid underflow) ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def compute_segment_starts(paths, start, end, lookback):\n",
        "    starts = []\n",
        "    idx = 0\n",
        "    start = pd.to_datetime(start); end = pd.to_datetime(end)\n",
        "    any_col = (PARAMS['features'] + ['y'])[0]\n",
        "    for p in paths:\n",
        "        df = pd.read_parquet(p, columns=[any_col]).sort_index()\n",
        "        df = df.loc[(df.index >= start) & (df.index <= end)]\n",
        "        n = max(0, len(df) - int(lookback))\n",
        "        if n > 0:\n",
        "            starts.append(idx)\n",
        "            idx += n\n",
        "    return np.array(starts, dtype=np.int64)\n",
        "\n",
        "starts_val  = compute_segment_starts(val_paths,  PARAMS['split']['val_start'],  PARAMS['split']['val_end'],  PARAMS['lookback'])\n",
        "starts_test = compute_segment_starts(test_paths, PARAMS['split']['test_start'], PARAMS['split']['test_end'], PARAMS['lookback'])\n",
        "\n",
        "def backtest_from_positions_segments(returns, positions, starts, cost_bps=5):\n",
        "    returns = returns.astype(np.float64)\n",
        "    positions = positions.astype(int)\n",
        "\n",
        "    # reset carry at each ticker start\n",
        "    pos_shift = np.roll(positions, 1)\n",
        "    pos_shift[0] = 0\n",
        "    if len(starts):\n",
        "        pos_shift[starts] = 0\n",
        "\n",
        "    trades = (pos_shift[1:] != pos_shift[:-1]).astype(int)\n",
        "    costs = np.zeros_like(returns, dtype=np.float64)\n",
        "    costs[1:] = trades * (cost_bps / 1e4)\n",
        "\n",
        "    daily_pnl = pos_shift * returns - costs\n",
        "\n",
        "    # log-equity to avoid underflow\n",
        "    le = np.cumsum(np.log1p(np.clip(daily_pnl, -0.999999, 10.0)))\n",
        "    equity = np.exp(le)\n",
        "\n",
        "    # drawdown from log-equity\n",
        "    peaks_log = np.maximum.accumulate(le)\n",
        "    maxdd = float(np.exp(le - peaks_log).min() - 1.0)\n",
        "\n",
        "    # metrics\n",
        "    sharpe = float((daily_pnl.mean() / (daily_pnl.std() + 1e-8)) * np.sqrt(252)) if daily_pnl.size else 0.0\n",
        "    years = daily_pnl.size / 252.0\n",
        "    cagr = (equity[-1] ** (1 / years) - 1.0) if (years > 0 and equity[-1] > 0) else 0.0\n",
        "\n",
        "    return equity, trades, daily_pnl, {\"CAGR\": float(cagr), \"MaxDD\": maxdd, \"Sharpe\": sharpe}\n",
        "\n",
        "# Use sanitized returns from earlier; if not present, fall back to raw\n",
        "rets_val_s  = globals().get(\"rets_val_s\",  rets_val)\n",
        "rets_test_s = globals().get(\"rets_test_s\", rets_test)\n",
        "\n",
        "eq_val,  trades_val,  pnl_val,  m_val  = backtest_from_positions_segments(rets_val_s,  pos_val,  starts_val,  cost_bps=cost_bps)\n",
        "eq_test, trades_test, pnl_test, m_test = backtest_from_positions_segments(rets_test_s, pos_test, starts_test, cost_bps=cost_bps)\n",
        "\n",
        "print(\"VAL  metrics (seg):\",  m_val,  \"| trades:\", int(trades_val.sum()))\n",
        "print(\"TEST metrics (seg):\", m_test, \"| trades:\", int(trades_test.sum()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3LkYq8_KJpAb",
        "outputId": "f86cd252-195e-4e6a-e85a-40dcd572937f"
      },
      "id": "3LkYq8_KJpAb",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VAL  metrics (seg): {'CAGR': -0.017960838711980398, 'MaxDD': -0.999999999999998, 'Sharpe': 0.015690154644168623} | trades: 1414\n",
            "TEST metrics (seg): {'CAGR': -0.05620217465915145, 'MaxDD': -1.0, 'Sharpe': -0.15914943293546552} | trades: 5394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def describe(arr, name):\n",
        "    q = np.quantile(arr, [0, 0.001, 0.01, 0.05, 0.5, 0.95, 0.99, 0.999, 1])\n",
        "    print(f\"{name}: min={q[0]:.6f}  q0.1%={q[1]:.6f}  q1%={q[2]:.6f}  q5%={q[3]:.6f}  \"\n",
        "          f\"median={q[4]:.6f}  q95%={q[5]:.6f}  q99%={q[6]:.6f}  q99.9%={q[7]:.6f}  max={q[8]:.6f}\")\n",
        "\n",
        "print(\"VAL returns:\")\n",
        "describe(rets_val, \"rets_val\")\n",
        "print(\"TEST returns:\")\n",
        "describe(rets_test, \"rets_test\")\n",
        "\n",
        "# Did PnL ever go below -100% in one day?\n",
        "print(\"\\nPnL sanity:\")\n",
        "print(\" min pnl_val:\", float(pnl_val.min()), \" <= -1 count:\", int((pnl_val <= -1).sum()))\n",
        "print(\" min pnl_test:\", float(pnl_test.min()), \" <= -1 count:\", int((pnl_test <= -1).sum()))\n",
        "\n",
        "# How many returns have absolute value > 1 (i.e., > 100% in *fractional* terms)?\n",
        "print(\"\\nabs(rets) > 1 counts (val/test):\",\n",
        "      int((np.abs(rets_val) > 1).sum()),\n",
        "      int((np.abs(rets_test) > 1).sum()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2CGPAAX20jCb",
        "outputId": "bf2aed1f-b324-4583-8d75-63a13c13aa76"
      },
      "id": "2CGPAAX20jCb",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VAL returns:\n",
            "rets_val: min=-0.995000  q0.1%=-0.268532  q1%=-0.080238  q5%=-0.036316  median=0.000000  q95%=0.035576  q99%=0.087408  q99.9%=0.334319  max=99.000000\n",
            "TEST returns:\n",
            "rets_test: min=-1.972603  q0.1%=-0.284789  q1%=-0.093154  q5%=-0.046595  median=0.000000  q95%=0.050514  q99%=0.115000  q99.9%=0.438158  max=3098.000000\n",
            "\n",
            "PnL sanity:\n",
            " min pnl_val: -0.30050001192092896  <= -1 count: 0\n",
            " min pnl_test: -0.30050001192092896  <= -1 count: 0\n",
            "\n",
            "abs(rets) > 1 counts (val/test): 83 397\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Buy & Hold baseline: equal-weight daily portfolio (streaming, tiny RAM) ---\n",
        "import os, json, numpy as np, pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def compute_metrics_from_equity_and_rets(equity, daily_rets):\n",
        "    if len(equity) == 0:\n",
        "        return {\"CAGR\": 0.0, \"MaxDD\": 0.0, \"Sharpe\": 0.0}\n",
        "    # log-space DD for numeric stability\n",
        "    le = np.log(np.clip(equity, 1e-12, None))\n",
        "    peaks_log = np.maximum.accumulate(le)\n",
        "    maxdd = float(np.exp(le - peaks_log).min() - 1.0)\n",
        "\n",
        "    years = len(daily_rets) / 252.0\n",
        "    cagr = (equity[-1] ** (1/years) - 1.0) if (years > 0 and equity[-1] > 0) else 0.0\n",
        "    sharpe = float((daily_rets.mean() / (daily_rets.std() + 1e-8)) * np.sqrt(252)) if daily_rets.size else 0.0\n",
        "    return {\"CAGR\": float(cagr), \"MaxDD\": maxdd, \"Sharpe\": sharpe}\n",
        "\n",
        "def equal_weight_bh(paths, start, end, cap=0.30):\n",
        "    start = pd.to_datetime(start); end = pd.to_datetime(end)\n",
        "    sums = {}   # date -> sum of returns\n",
        "    counts = {} # date -> ticker count that day\n",
        "\n",
        "    for p in paths:\n",
        "        # read only the forward return; index holds the date\n",
        "        df = pd.read_parquet(p, columns=['ret_fwd_1']).sort_index()\n",
        "        df = df.loc[(df.index >= start) & (df.index <= end)]\n",
        "        if df.empty:\n",
        "            continue\n",
        "        r = sanitize_returns(df['ret_fwd_1'].to_numpy(dtype=np.float32), cap=cap)\n",
        "\n",
        "        # accumulate by date (normalize index to date only)\n",
        "        for d, ri in zip(df.index, r):\n",
        "            day = pd.Timestamp(d).normalize()  # midnight of that day\n",
        "            sums[day] = sums.get(day, 0.0) + float(ri)\n",
        "            counts[day] = counts.get(day, 0) + 1\n",
        "\n",
        "    if not sums:\n",
        "        return [], np.array([]), np.array([]), {\"CAGR\": 0.0, \"MaxDD\": 0.0, \"Sharpe\": 0.0}\n",
        "\n",
        "    dates = sorted(sums.keys())\n",
        "    daily_rets = np.array([sums[d] / max(counts[d], 1) for d in dates], dtype=np.float64)\n",
        "    equity = np.cumprod(1.0 + daily_rets)\n",
        "    metrics = compute_metrics_from_equity_and_rets(equity, daily_rets)\n",
        "    return dates, daily_rets, equity, metrics\n",
        "\n",
        "# Run baseline on val & test\n",
        "val_dates,  val_rets_bh,  val_eq_bh,  m_val_bh  = equal_weight_bh(\n",
        "    val_paths,  PARAMS['split']['val_start'],  PARAMS['split']['val_end'],  cap=0.30\n",
        ")\n",
        "test_dates, test_rets_bh, test_eq_bh, m_test_bh = equal_weight_bh(\n",
        "    test_paths, PARAMS['split']['test_start'], PARAMS['split']['test_end'], cap=0.30\n",
        ")\n",
        "\n",
        "print(\"B&H VAL : days=\", len(val_dates),  \"| metrics:\",  m_val_bh)\n",
        "print(\"B&H TEST: days=\", len(test_dates), \"| metrics:\", m_test_bh)\n",
        "\n",
        "# Save artifacts\n",
        "def plot_equity(eq, title, path):\n",
        "    plt.figure()\n",
        "    plt.plot(eq)\n",
        "    plt.title(title); plt.xlabel(\"Days\"); plt.ylabel(\"Equity\")\n",
        "    plt.grid(True); plt.savefig(path, dpi=150, bbox_inches='tight'); plt.close()\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "val_png  = os.path.join(OUTPUT_DIR, \"baseline_equity_val.png\")\n",
        "test_png = os.path.join(OUTPUT_DIR, \"baseline_equity_test.png\")\n",
        "plot_equity(val_eq_bh,  \"Buy & Hold (Equal-Weight) — Validation\", val_png)\n",
        "plot_equity(test_eq_bh, \"Buy & Hold (Equal-Weight) — Test\",       test_png)\n",
        "\n",
        "baseline_json = {\n",
        "    \"seed\": SEED,\n",
        "    \"params\": PARAMS,\n",
        "    \"baseline\": \"equal_weight_buy_and_hold\",\n",
        "    \"val\": {\"days\": len(val_dates),  \"metrics\": m_val_bh},\n",
        "    \"test\":{\"days\": len(test_dates), \"metrics\": m_test_bh},\n",
        "}\n",
        "with open(os.path.join(OUTPUT_DIR, \"baseline_run.json\"), \"w\") as f:\n",
        "    json.dump(baseline_json, f, indent=2)\n",
        "\n",
        "print(\"Saved baseline:\",\n",
        "      val_png, \",\",\n",
        "      test_png, \",\",\n",
        "      os.path.join(OUTPUT_DIR, \"baseline_run.json\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvQNavioabtn",
        "outputId": "2eafa95a-3ea0-4165-cd7b-4c7ad232eddb"
      },
      "id": "lvQNavioabtn",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "B&H VAL : days= 252 | metrics: {'CAGR': 0.2231077131275303, 'MaxDD': -0.08114500414961767, 'Sharpe': 1.8023377145278825}\n",
            "B&H TEST: days= 743 | metrics: {'CAGR': 0.08300290961302337, 'MaxDD': -0.4115523167155214, 'Sharpe': 0.4384846621035782}\n",
            "Saved baseline: /content/drive/MyDrive/Colab Notebooks/outputs/baseline_equity_val.png , /content/drive/MyDrive/Colab Notebooks/outputs/baseline_equity_test.png , /content/drive/MyDrive/Colab Notebooks/outputs/baseline_run.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Temperature scaling (calibrate on VAL) + re-evaluate model portfolio (VAL & TEST) ---\n",
        "import os, numpy as np, pandas as pd, torch, json\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# ensure normalization stats are loaded in this cell's scope\n",
        "if 'FEAT_MEAN' not in globals():\n",
        "    norm_df = pd.read_csv(os.path.join(OUTPUT_DIR, \"feature_norm.csv\"), index_col=0)\n",
        "    FEAT_MEAN = norm_df[\"mean\"].astype(float).to_dict()\n",
        "    FEAT_STD  = norm_df[\"std\"].replace(0, 1.0).astype(float).to_dict()\n",
        "\n",
        "# Use your existing eval dataset + infer helper if present; else define minimal versions\n",
        "try:\n",
        "    TickerEvalIterableDataset\n",
        "    infer_probs_and_returns\n",
        "except NameError:\n",
        "    from torch.utils.data import IterableDataset, DataLoader\n",
        "    class TickerEvalIterableDataset(IterableDataset):\n",
        "        def __init__(self, parquet_paths, start, end, features, lookback):\n",
        "            self.paths = parquet_paths\n",
        "            self.start = pd.to_datetime(start)\n",
        "            self.end   = pd.to_datetime(end)\n",
        "            self.features = features\n",
        "            self.lookback = int(lookback)\n",
        "        def __iter__(self):\n",
        "            for p in self.paths:\n",
        "                df = pd.read_parquet(p, columns=self.features + ['y','ret_fwd_1']).sort_index()\n",
        "                df = df.loc[(df.index >= self.start) & (df.index <= self.end)]\n",
        "                if len(df) < self.lookback + 2:\n",
        "                    continue\n",
        "                # same normalization as training\n",
        "                for c in self.features:\n",
        "                    df[c] = (df[c] - FEAT_MEAN[c]) / FEAT_STD[c]\n",
        "                X = df[self.features].to_numpy(dtype=np.float32, copy=False)\n",
        "                y = df['y'].to_numpy(dtype=np.int64, copy=False)\n",
        "                r = df['ret_fwd_1'].to_numpy(dtype=np.float32, copy=False)\n",
        "                for i in range(self.lookback, len(df)):\n",
        "                    yield X[i-self.lookback:i], y[i], r[i]\n",
        "    def infer_probs_and_returns(model, loader, device):\n",
        "        model.eval(); probs=[]; rets=[]; labels=[]\n",
        "        with torch.no_grad():\n",
        "            for xb, yb, rb in loader:\n",
        "                xb = xb.to(device, non_blocking=True)\n",
        "                p1 = torch.softmax(model(xb), dim=1)[:,1].cpu().numpy()\n",
        "                probs.append(p1); rets.append(rb.numpy()); labels.append(yb.numpy())\n",
        "        if not probs:\n",
        "            return np.array([]), np.array([]), np.array([])\n",
        "        return np.concatenate(probs), np.concatenate(rets), np.concatenate(labels)\n",
        "\n",
        "# Build or reuse loaders\n",
        "from torch.utils.data import DataLoader\n",
        "feat_cols = PARAMS['features']; lb = int(PARAMS['lookback'])\n",
        "def make_eval_loader(paths, start, end, bs=4096):\n",
        "    ds = TickerEvalIterableDataset(paths, start, end, feat_cols, lb)\n",
        "    return DataLoader(ds, batch_size=bs, shuffle=False, num_workers=0, pin_memory=True)\n",
        "\n",
        "val_eval_loader = globals().get(\"val_eval_loader\", None)\n",
        "if val_eval_loader is None:\n",
        "    val_eval_loader = make_eval_loader(val_paths, PARAMS['split']['val_start'], PARAMS['split']['val_end'])\n",
        "\n",
        "test_eval_loader = globals().get(\"test_eval_loader\", None)\n",
        "if test_eval_loader is None:\n",
        "    test_eval_loader = make_eval_loader(test_paths, PARAMS['split']['test_start'], PARAMS['split']['test_end'])\n",
        "\n",
        "# Load model\n",
        "model = build_model(PARAMS['model_option'], in_feats=len(feat_cols)).to(device).eval()\n",
        "state = torch.load(os.path.join(OUTPUT_DIR, \"best_model.pt\"), map_location=device)\n",
        "sd = state if isinstance(state, dict) and 'model' not in state else state.get('model', state)\n",
        "model.load_state_dict(sd)\n",
        "\n",
        "# 1) Collect VAL probs + labels for calibration\n",
        "p_val, r_dummy, y_val = infer_probs_and_returns(model, val_eval_loader, device)\n",
        "eps = 1e-6\n",
        "p_val = np.clip(p_val, eps, 1.0 - eps)\n",
        "logit_val = np.log(p_val / (1.0 - p_val))\n",
        "\n",
        "# 2) Fit temperature by minimizing log loss on VAL (1D grid, robust & fast)\n",
        "def nll_at_T(T):\n",
        "    z = logit_val / T\n",
        "    p = 1.0 / (1.0 + np.exp(-z))\n",
        "    # binary log loss\n",
        "    return -np.mean(y_val * np.log(np.clip(p, eps, 1-eps)) + (1-y_val) * np.log(np.clip(1-p, eps, 1-eps)))\n",
        "\n",
        "grid = np.geomspace(0.25, 4.0, 40)\n",
        "losses = np.array([nll_at_T(T) for T in grid])\n",
        "T_best = float(grid[np.argmin(losses)])\n",
        "print(f\"Best temperature on VAL: T={T_best:.3f} (vs uncalibrated T=1.0)\")\n",
        "\n",
        "# Small helper: apply temperature to any prob vector\n",
        "def apply_temp(p, T):\n",
        "    p = np.clip(p, eps, 1.0 - eps)\n",
        "    z = np.log(p / (1.0 - p)) / T\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "# 3) Rebuild model portfolio with calibrated probs (equal-weight, date-aligned)\n",
        "\n",
        "def portfolio_with_temp(paths, start, end, margin, cost_bps=5, cap=0.30, batch_windows=4096):\n",
        "    # norms\n",
        "    norm_df = pd.read_csv(os.path.join(OUTPUT_DIR, \"feature_norm.csv\"), index_col=0)\n",
        "    FEAT_MEAN = norm_df[\"mean\"].astype(float).to_dict()\n",
        "    FEAT_STD  = norm_df[\"std\"].replace(0, 1.0).astype(float).to_dict()\n",
        "\n",
        "    start = pd.to_datetime(start); end = pd.to_datetime(end)\n",
        "    sum_pnl, count = {}, {}\n",
        "    th_long, th_short = 0.5 + margin, 0.5 - margin\n",
        "\n",
        "    for pth in paths:\n",
        "        try:\n",
        "            df = pd.read_parquet(pth, columns=feat_cols + ['ret_fwd_1']).sort_index()\n",
        "        except Exception:\n",
        "            continue\n",
        "        df = df.loc[(df.index >= start) & (df.index <= end)]\n",
        "        if len(df) < lb + 2:\n",
        "            continue\n",
        "        for c in feat_cols:\n",
        "            if c in df.columns:\n",
        "                df[c] = (df[c] - FEAT_MEAN[c]) / FEAT_STD[c]\n",
        "\n",
        "        X = df[feat_cols].to_numpy(dtype=np.float32, copy=False)\n",
        "        r = sanitize_returns(df['ret_fwd_1'].to_numpy(dtype=np.float32, copy=False), cap=cap)\n",
        "\n",
        "        # batched window inference\n",
        "        Tlen = len(df)\n",
        "        probs = []\n",
        "        with torch.no_grad():\n",
        "            for i in range(lb, Tlen, batch_windows):\n",
        "                j = min(Tlen, i + batch_windows)\n",
        "                windows = np.stack([X[k-lb:k] for k in range(i, j)], axis=0).astype(np.float32)\n",
        "                xb = torch.from_numpy(windows).to(device)\n",
        "                p1 = torch.softmax(model(xb), dim=1)[:,1].detach().cpu().numpy()\n",
        "                probs.append(p1)\n",
        "        if not probs:\n",
        "            continue\n",
        "        probs = np.concatenate(probs)\n",
        "        probs = apply_temp(probs, T_best)  # <-- calibration applied here\n",
        "\n",
        "        r_lbl = r[lb:]\n",
        "        dates = df.index[lb:]\n",
        "\n",
        "        pos = np.zeros_like(probs, dtype=np.int8)\n",
        "        pos[probs >= th_long] = 1\n",
        "        pos[probs <= th_short] = -1\n",
        "\n",
        "        pos_shift = np.roll(pos.astype(int), 1); pos_shift[0] = 0\n",
        "        trades = (pos_shift[1:] != pos_shift[:-1]).astype(int)\n",
        "        costs  = np.zeros_like(r_lbl, dtype=np.float32)\n",
        "        costs[1:] = trades * (cost_bps / 1e4)\n",
        "\n",
        "        pnl = pos_shift * r_lbl - costs\n",
        "        for d, x in zip(dates, pnl):\n",
        "            day = pd.Timestamp(d).normalize()\n",
        "            sum_pnl[day] = sum_pnl.get(day, 0.0) + float(x)\n",
        "            count[day]   = count.get(day, 0) + 1\n",
        "\n",
        "    if not sum_pnl:\n",
        "        return [], np.array([]), np.array([]), {\"CAGR\": 0.0, \"MaxDD\": 0.0, \"Sharpe\": 0.0}\n",
        "\n",
        "    days = sorted(sum_pnl.keys())\n",
        "    daily_rets = np.array([sum_pnl[d] / max(count[d],1) for d in days], dtype=np.float64)\n",
        "    equity = np.cumprod(1.0 + daily_rets)\n",
        "\n",
        "    # metrics\n",
        "    le = np.log(np.clip(equity, 1e-12, None))\n",
        "    peaks_log = np.maximum.accumulate(le)\n",
        "    maxdd = float(np.exp(le - peaks_log).min() - 1.0)\n",
        "    years = len(daily_rets) / 252.0\n",
        "    cagr  = (equity[-1] ** (1/years) - 1.0) if (years > 0 and equity[-1] > 0) else 0.0\n",
        "    sharpe = float((daily_rets.mean() / (daily_rets.std() + 1e-8)) * np.sqrt(252)) if daily_rets.size else 0.0\n",
        "    return days, daily_rets, equity, {\"CAGR\": float(cagr), \"MaxDD\": maxdd, \"Sharpe\": sharpe}\n",
        "\n",
        "# Use the chosen margin from your sweep (0.15)\n",
        "margin = 0.02\n",
        "val_days,  val_rets_c,  val_eq_c,  m_val_c  = portfolio_with_temp(val_paths,  PARAMS['split']['val_start'],  PARAMS['split']['val_end'],  margin, cost_bps=PARAMS.get('cost_bps',5))\n",
        "test_days, test_rets_c, test_eq_c, m_test_c = portfolio_with_temp(test_paths, PARAMS['split']['test_start'], PARAMS['split']['test_end'], margin, cost_bps=PARAMS.get('cost_bps',5))\n",
        "\n",
        "print(f\"CALIBRATED (T={T_best:.3f}) VAL  metrics @ margin={margin}: \", m_val_c)\n",
        "print(f\"CALIBRATED (T={T_best:.3f}) TEST metrics @ margin={margin}: \", m_test_c)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-h1IkSGdkY8H",
        "outputId": "417c97be-4ae1-47e4-8a36-83254fa1f17c"
      },
      "id": "-h1IkSGdkY8H",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best temperature on VAL: T=0.899 (vs uncalibrated T=1.0)\n",
            "CALIBRATED (T=0.899) VAL  metrics @ margin=0.02:  {'CAGR': -0.026403907520209935, 'MaxDD': -0.06667088128018483, 'Sharpe': -0.49574412114335936}\n",
            "CALIBRATED (T=0.899) TEST metrics @ margin=0.02:  {'CAGR': -0.11699866071416731, 'MaxDD': -0.32370792794089076, 'Sharpe': -0.9384402698914333}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set in-memory settings for this session\n",
        "PARAMS['portfolio_mode'] = 'long_only'\n",
        "PARAMS['margin'] = 0.02          # <— your chosen value\n",
        "PARAMS['calibration_T'] = 0.899  # from temp scaling\n",
        "# optional:\n",
        "# PARAMS['cost_bps'] = 5\n",
        "print(\"Session settings set →\", {k: PARAMS[k] for k in ['portfolio_mode','margin','calibration_T']})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w36ysEjyGLDV",
        "outputId": "e193004c-acd6-48ee-f43e-7b1cad43175d"
      },
      "id": "w36ysEjyGLDV",
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Session settings set → {'portfolio_mode': 'long_only', 'margin': 0.02, 'calibration_T': 0.899}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure we’re using YOUR chosen settings before running the saver\n",
        "# (Pull from PARAMS, or hard-set them here)\n",
        "T_best = float(PARAMS.get('calibration_T', 0.899))\n",
        "margin = float(PARAMS.get('margin', 0.02))       # <-- you found 0.02 works\n",
        "mode   = PARAMS.get('portfolio_mode', 'long_only')\n",
        "\n",
        "print(f\"Using settings → mode={mode}, margin={margin}, T={T_best}, cost_bps={PARAMS.get('cost_bps',5)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUvcedIyCmZc",
        "outputId": "e6c75820-e170-432e-c4ab-b855d64f8b31"
      },
      "id": "vUvcedIyCmZc",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using settings → mode=long_only, margin=0.02, T=0.899, cost_bps=5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Lock portfolio settings ---\n",
        "import json, os\n",
        "PARAMS['calibration_T'] = float(globals().get('T_best', 1.0))  # 0.899 for you\n",
        "PARAMS['portfolio_mode'] = 'long_only'\n",
        "PARAMS['margin'] = 0.02  # or whatever you pick\n",
        "lock_path = os.path.join(OUTPUT_DIR, 'params_locked.json')\n",
        "with open(lock_path, 'w') as f:\n",
        "    json.dump(PARAMS, f, indent=2)\n",
        "print(f\"Using settings → mode={mode}, margin={margin}, T={T_best}, cost_bps={PARAMS.get('cost_bps',5)}\")\n",
        "print(\"Saved:\", lock_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JEElpGrTz_dh",
        "outputId": "48ed61ae-8df4-488b-fc41-d27bfa291c88"
      },
      "id": "JEElpGrTz_dh",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using settings → mode=long_only, margin=0.02, T=0.899, cost_bps=5\n",
            "Saved: /content/drive/MyDrive/Colab Notebooks/outputs/params_locked.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure saver uses locked settings\n",
        "T_best = float(PARAMS.get('calibration_T', 1.0))\n",
        "margin = float(PARAMS.get('margin', 0.02))\n",
        "mode   = PARAMS.get('portfolio_mode', 'long_only')\n",
        "print(f\"Using settings → mode={mode}, margin={margin}, T={T_best}, cost_bps={PARAMS.get('cost_bps',5)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o45Te26oFI97",
        "outputId": "cc4ecd3a-9015-4467-b613-130d4aba3cca"
      },
      "id": "o45Te26oFI97",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using settings → mode=long_only, margin=0.02, T=0.899, cost_bps=5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdOv-D3eMcZH"
      },
      "source": [
        "## Metrics & plots"
      ],
      "id": "rdOv-D3eMcZH"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_b-YQxeMcZH"
      },
      "outputs": [],
      "source": [
        "def plot_equity(equity_curve, title='Equity Curve', path=os.path.join(OUTPUT_DIR, 'equity_curve.png')):\n",
        "    plt.figure()\n",
        "    plt.plot(equity_curve)\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Days')\n",
        "    plt.ylabel('Equity')\n",
        "    plt.grid(True)\n",
        "    plt.savefig(path, dpi=150, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    return path\n",
        "\n",
        "print('Metrics/plots stubs ready.')\n"
      ],
      "id": "x_b-YQxeMcZH"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6HEYApwMcZH"
      },
      "source": [
        "## Params dump & outputs"
      ],
      "id": "j6HEYApwMcZH"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fPlXPOx5McZI"
      },
      "outputs": [],
      "source": [
        "def save_run(seed, params, metrics: dict):\n",
        "    run = {\n",
        "        'timestamp': dt.datetime.utcnow().isoformat() + 'Z',\n",
        "        'seed': seed,\n",
        "        'params': params,\n",
        "        'metrics': metrics,\n",
        "    }\n",
        "    with open(os.path.join(OUTPUT_DIR, 'run.json'), 'w') as f:\n",
        "        json.dump(run, f, indent=2)\n",
        "    print('Saved outputs/run.json')\n",
        "\n",
        "print('Output saving stub ready.')\n"
      ],
      "id": "fPlXPOx5McZI"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHc4l_bjMcZI"
      },
      "source": [
        "---\n",
        "### Wiring plan (streaming + Bootstrap; in this order)\n",
        "1) Setup & seed\n",
        "   - Imports, SEED, paths (PARQUET_DIR, OUTPUT_DIR), PARAMS defined.\n",
        "\n",
        "2) Bootstrap A (resume-safe; no model deps)\n",
        "   - Mount Drive.\n",
        "   - Load params_locked.json (if present) and merge into PARAMS.\n",
        "   - Load feature_norm.csv → define normalize_inplace.\n",
        "   - Build train_paths / val_paths / test_paths using lookback+2 filter.\n",
        "\n",
        "3) (Optional) Local overrides for this session\n",
        "   - e.g., portfolio_mode, margin, calibration_T. Only persisted if you choose to lock later.\n",
        "\n",
        "4) Model definition\n",
        "   - Define build_model(...). No training here.\n",
        "\n",
        "5) Bootstrap B (after model exists)\n",
        "   - Load best_model.pt if present; set device; expose model_probs(...).\n",
        "   - Quick smoke test on 1 ticker to verify shapes.\n",
        "\n",
        "6) (Optional) Training (single, stabilized loop)\n",
        "   - Use the streaming TickerIterableDataset + DataLoader.\n",
        "   - Save checkpoint to outputs/best_model.pt when improved.\n",
        "\n",
        "7) Inference & backtest utilities (single canonical set)\n",
        "   - sanitize_returns, positions_from_probs, compute_metrics, etc.\n",
        "\n",
        "8) Evaluation\n",
        "   - Run VAL (then TEST) backtests with costs; choose long_only or long_short per PARAMS.\n",
        "   - (Optional) Temperature scaling on VAL; update PARAMS['calibration_T'] when happy.\n",
        "\n",
        "9) Exports\n",
        "   - Save metrics, equity/trades CSVs, and a concise run.json.\n",
        "   - Lock params to params_locked.json only when you intend to “freeze” a run.\n",
        "\n",
        "10) Utilities (run once; guarded)\n",
        "   - CSV→Parquet builder, yfinance fetches, or norms regeneration from train_paths.\n",
        "   - Keep behind RUN_ONCE to avoid accidental side effects.\n"
      ],
      "id": "NHc4l_bjMcZI"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "RwhtxRzKprt3"
      ],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}